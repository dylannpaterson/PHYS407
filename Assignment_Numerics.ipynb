{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignment module: \n",
    "# numerical computation with *python*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#please ignore this -- it's a \"special\" command \n",
    "#that we only need to enable plotting within the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In this session we will take a look at some of the more common types of numerical tasks we might want to accomplish with a computer program:\n",
    "\n",
    "* integration\n",
    "* differentiation\n",
    "* root finding and minimization\n",
    "* interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "I will keep keep my promise from the start of the course that we would not be delving into the detailed mathematics of such things, nor into advanced and clever algorithms for them.\n",
    "\n",
    "What we'll do instead is:\n",
    "\n",
    "1. develop a simple conceptual understanding of these tasks, so that we can use existing algorithms correctly and competently\n",
    "\n",
    "2. introduce some of the basic functionality provided by *scipy* to accomplish these tasks. \n",
    "\n",
    "Note that the documentation for *scipy* includes an excellent <a href=\"http://docs.scipy.org/doc/scipy/reference/tutorial/index.html\">tutorial</a>, which includes examples for all of the functions we'll be discussing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## integration\n",
    "\n",
    "The definite integral of a function between two limits is equivalent to the area under the curve defined by that function.\n",
    "\n",
    "<div align=\"center\"><img src=\"area_u14.gif\"></div>\n",
    "\n",
    "<div align=\"right\">  <a href=\"http://www.teacherschoice.com.au/maths_library/calculus/area_under_a_curve.htm\">source of image</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The simplest possible way to estimate that area numerically is to break it up into little rectangles, centered on a set of discrete points, $x_i^*$:\n",
    "\n",
    "<div align=\"center\"><img src=\"recint.gif\"></div>\n",
    "<div align=\"right\">  <a href=\"http://www.teacherschoice.com.au/maths_library/calculus/area_under_a_curve.htm\">source of image</div></a>\n",
    "\n",
    "This immediately suggest a simple numerical algorithm for calculating definite integrals:<br><br>\n",
    "\n",
    "<div align=\"center\"> \n",
    "$\\int_a^b f(x) dx \\simeq \\sum_i^N f(x_i^*) \\Delta x$ ,\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "where $\\Delta x$ is the width of the little rectangles, and $N$ is the total number we need to tile the area of the function we care about. \n",
    "\n",
    "This approximation gets mathematically better and better as we make $\\Delta x$ smaller and smaller. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The key quantity here is $\\Delta x$, which needs to be:\n",
    "* small enough for the approximation to be sufficiently accurate\n",
    "* large enough not produce round-off errors\n",
    "    * the computer has to be able to tell the difference between $x_i$ and $x_i + \\Delta_x$;\n",
    "* large enough so that the sum doesn't take too long to compute\n",
    "\n",
    "Virtually all numerical integration routines are essentially elaborations on this basic idea. \n",
    "\n",
    "The same trade-offs apply to all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In order to gain some insight, let's write a function that does a really simple numerical integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integral f(x) =  0.333333333334\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    y = x**2.0\n",
    "    return y\n",
    "\n",
    "def recint(f,start,stop,dx):\n",
    "    sum = 0.0\n",
    "    x = start + 0.5*dx\n",
    "    while (x < stop):\n",
    "        sum = sum + f(x)\n",
    "        x = x + dx\n",
    "    return sum*dx\n",
    "\n",
    "print \"Integral f(x) = \",recint(f,0.0,1.0,0.000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "##### Exercise\n",
    "\n",
    "Write a program that calls *recint()* for a wide range of values for dx. Plot the error -- i.e. the difference between the approximate estimate returned by *recint()* and the analytical result -- as a function of dx.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### adaptive integration\n",
    "\n",
    "In most real-world applications, we do not know the analytical result (if we did we wouldn't be doing a numerical integration). So it would be good to have a purely numerical way to estimate the accuracy of our estimate. \n",
    "\n",
    "The easiest way to accomplish this is by calculating the integral for two values of $dx$ (e.g. $dx_1$ and $dx_2 = 0.5 dx_1$). The difference between the two is at least a rough estimate of the error on the integral calculated with $dx_1$. \n",
    "\n",
    "This immediately suggests how we might desigve for $dx_1$ and $dx_2 = 0.5 dx_1$. If the (absolute value of the) difference is larger than our desired level of accuracy, we set $dx_3 = 0.5 dx_2$ and run it again. We then repeat this until the difference between the results for $dx_i$ and $dx_{i+1}$ is smaller than our desired accuracy. At that point we return the result obtained with $dx_{i+1}$. \n",
    "\n",
    "Such algorithms are called *adaptive*, since they adaptively adjust the precision to which they work to yield a result of the required accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "##### Exercise\n",
    "\n",
    "Take the simple *rectint()* algorithm above and turn it into an adaptive one. Then test that it really produces results to the desired accuracy on a range of trial functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### numerical integration with *scipy*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "So how do we actually do numerical integrations within *Python* (assuming we don't want to rely on our own simple integrators, like the one above)?\n",
    "\n",
    "Easy, we use *scipy*!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "*scipy* contains an entire module <a href=\"http://docs.scipy.org/doc/scipy/reference/tutorial/integrate.html\"> integrate</a> that provides several functions for numerical integration.\n",
    "\n",
    "There are two basic types of integrator functions provided:\n",
    "\n",
    "* ones that take a function as input and decide for themselves at what locations to call this function:\n",
    "    * the standard work-horse function of this type is <a href=http://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.quad.html#scipy.integrate.quad> quad();</a>\n",
    "\n",
    "\n",
    "* ones that take pre-defined *samples* of a function as input (i.e. the values at a set of pre-determined locations):\n",
    "    * a good general-purpose integrator of this type is <a href=http://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.simps.html#scipy.integrate.simps> simps()</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "##### Exercise\n",
    "\n",
    "Read the documentation for these *scipy* integrators and make sure you understand how to use them. Pay particular attention to which parameters are *required* and which are *optional*. \n",
    "\n",
    "##### Exercise\n",
    "\n",
    "Write a program that uses our own simple (adaptive) integrator as well as *quad()* on a given function and compares the results. \n",
    "\n",
    "##### Exercise\n",
    "\n",
    "Create samples for the same function and use the *simps()* integrator to integrate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## differentiation\n",
    "\n",
    "The derivative of a function at some point $a$ is the local slope of the curve defined by the function at $a$. This means it's the slope of the tangent line to the curve at point $a$.\n",
    "\n",
    "<div align=\"center\"> <img src=\"Deriv2.png\"></div>\n",
    "\n",
    "<div align=\"right\">  <a href=\"http://hyperphysics.phy-astr.gsu.edu/hbase/math/derint.html\">source of image</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This immediately suggests a way to estimate the derivative at point $a$ by taking two points on the curve on either side of $a$ and estimating the slope of the line that connects them.\n",
    "\n",
    "As long as the points are \"close enough\" to $a$, so that the curve is well-approximated by a straight line, this estimate should be pretty good.\n",
    "\n",
    "\n",
    "<div align=\"center\"> <img src=\"Derivative2.png\"></div>\n",
    "\n",
    "<div align=\"right\">  <a href=\"http://hyperphysics.phy-astr.gsu.edu/hbase/deriv.html#c1\">source of image</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A reasonable numerical approximation to the derivative should therefore be given by:<br><br>\n",
    "\n",
    "<div align=\"center\"> \n",
    "$\\frac{df}{dx}_{x=a} \\simeq \\frac{f(a + 0.5\\Delta x) \\, - \\, f(a - 0.5\\Delta x)}{\\Delta x}$\n",
    "</div>\n",
    "\n",
    "as long as $\\Delta x$ is \"small enough\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's write a function that provides a simple numerical derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The derivative of f(x) at x = 1.0 is df/dx = 0.500000000158\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    y = x**0.5\n",
    "    return y\n",
    "\n",
    "def deriv(f,a,dx):\n",
    "    dy = f(a + 0.5*dx) - f(a - 0.5*dx)\n",
    "    return dy/dx\n",
    "\n",
    "a= 1.0\n",
    "dx = 1.e-4\n",
    "print \"The derivative of f(x) at x =\", a, \"is df/dx =\", deriv(f,a,dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Virtually all numerical differentiation algorithms are elaborations on this idea. For example, some more advanced routines might estimate the local slope using more than just two points and/or functions other than just straight line to connect these points. But the basic idea remaines the same.\n",
    "\n",
    "As for integration, the accuracy of numerical differentiation is fundementally set by the size of the $dx$ step:\n",
    "* if $dx$ is too large, the approximation breaks down; \n",
    "* if $dx$ is too small, we might reach the limits of machine precision\n",
    "    * i.e. the computer can no longer tell the difference between $x$ and $x + dx$, or between $f(x)$ and $f(x + dx)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### adaptive differentiation\n",
    "\n",
    "As with integration, it's fairly easy to see how numerical differentation can be made \"adaptive\", so that it provides a result that is \"guaranteed\" to be good to some user-defined accuracy. \n",
    "\n",
    "The process is virtually equivalent as for adaptive integration: we start with two reasonably large trial values for $dx$, compare the result, and then iteratively reduce $dx$ until the result for the last two trials agrees to within the desired level of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "##### Exercise\n",
    "\n",
    "Write a program that calls *deriv()* for a wide range of values for $dx$. Plot the error -- i.e. the difference between the approximate estimate returned by *deriv()* and the analytical result -- as a function of $dx$.\n",
    "\n",
    "##### Exercise\n",
    "\n",
    "Repeat the exercise, but now for several different functions for $f(x)$. Think about what conditions determine the level of accuracy provided by this type of numerical differentiation with a given value of $dx$.\n",
    "\n",
    "##### Exercise\n",
    "\n",
    "Take the simple *deriv()* algorithm above and turn it into an adaptive one. Then test that it really produces results to the desired accuracy on a range of trial functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### numerical derivatives with *scipy*\n",
    "\n",
    "There are not too many options for estimating derivatives provided natively in *scipy*.\n",
    "\n",
    "The standard go-to option is the function <a href=http://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.derivative.html#scipy.misc.derivative> derivative()</a> in the module <a href=http://docs.scipy.org/doc/scipy/reference/misc.html>misc</a> (which contains a collection of \"miscellaneous\" routines). \n",
    "\n",
    "The function *derivative()* uses an algorithm that's very similar to our simple *deriv()*, but \n",
    "is capable of estimating higher-order derivatives as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The same *misc* module also contains a function <a href=http://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.central_diff_weights.html#scipy.misc.central_diff_weights> central_diff_weights()</a>. The values it returns can be used to construct numerical derivative that are based on more than just two point around the desired location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "##### Exercise\n",
    "\n",
    "Read the documentation for these *scipy* functions and make sure you understand how to use them. Pay particular attention to which parameters are *required* and which are *optional*. \n",
    "\n",
    "##### Exercise\n",
    "\n",
    "Write a program that uses our own simple (adaptive) differentiation routine and also *scipy's* *derivative()* routine on a given function and compares the results. \n",
    "\n",
    "##### Exercise\n",
    "\n",
    "Add to your program another numerical derivative routine that uses the weights provided by *central_diff_weights()* to estimate the derivative to your function. Test how this estimate depends on the number of points on which the estimate is based. Are more points always better? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## minimization and maximization\n",
    "\n",
    "One of the more common problems that require a numerical solution is to find the minimum, the maximum or the root of a function.\n",
    "\n",
    "A *numerical* solution may be required for two reasons:\n",
    "\n",
    "* the analytical form of the function may not be known\n",
    "    * e.g. the function itself may be the product of a complex numerical calculation\n",
    "\n",
    "* there problem does not have a closed-form analytical solution\n",
    "    * e.g. try finding the root of $f(x) = x e^{x} - 7$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### minimization and maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The first thing to note here is that minimization and maximization are clearly the same problem. \n",
    "\n",
    "Suppose we know how to find the minimum of any function $f(x)$.\n",
    "\n",
    "We can then find the maximum of any function $g(x)$ just by setting $f(x) = -g(x)$ and minimizing $f(x)$.\n",
    "\n",
    "We can therefore focus exlusively on minimization below, without any loss of generality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose we need to locate the location of the minimum of a function like this (the blue dashed line just shows the y = 0 level):\n",
    "\n",
    "<div align=\"center\"><img width=50% height=50% src=\"function.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's assume that we don't know the analytical form of this function, but that we know how to calculate $f(x)$ numerically for any given $x$. \n",
    "\n",
    "How do we go about this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "There are three main classes of methods for solving this kind of problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### option 1: brute-force gridding\n",
    "\n",
    "The simplest minimimzation method is to construct a broad, densely spaced grid in $x$ and calculate $f(x)$ at each grid point.\n",
    "\n",
    "* our final estimate is then simply the grid point corresponding to the lowest value of $f(x)$<br><br>\n",
    "* advantages: \n",
    "    * easy to understand\n",
    "    * trivial to code\n",
    "    * extremely robust\n",
    "    * works even if there are multiple local minima\n",
    "    * no need for derivatives\n",
    "<br><br>\n",
    "* disadvantages\n",
    "    * extremely expensive for functions in many dimensions\n",
    "        * $n$ grid points in each of $d$ dimension $\\rightarrow$ $n_{tot} = n^d$\n",
    "        * e.g. $n = 50$, $d = 5$ $\\rightarrow$ $n_{tot} \\simeq 3 \\times 10^8$ !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "##### Exercise\n",
    "\n",
    "The function and plot shown above were generated by the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEKCAYAAAASByJ7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XmYVNWd//H3l0VwI0rEjUU2UUBQpNEAGXaNIhqDMeMeohEnxqiJOqO/ZBh1kolxnjGJOuMyZsbEJeoEFLdIBMUdw6KoLLLazS4IIshmy/n98e1KN/RW3V33nqruz+t56rnVfavu/bY29elzzj3nWggBERGRZrELEBGR/KBAEBERQIEgIiJlFAgiIgIoEEREpIwCQUREAAWCiIiUUSCIiAigQBARkTItYhdQF4ccckjo3Llz7DJERArK7NmzN4QQ2tX2uoIKhM6dOzNr1qzYZYiIFBQzK87mdeoyEhERQIEgIiJlFAgiIgIoEEREpIwCQUREAAWCiIiUUSCIiAhQYPMQRArG1q3w6KOwYwecdx4cemjsikRqpUAQybW1a2HIEFi82L++9Vb4y1/gxBPj1iVSC3UZieRSCN4iWLUKXnwR3nsP9t8fzj4bPv00dnUiNVIgiOTSH/8Ir7wCv/kNjBoFffrAn/4Eq1fDv/xL7OpEaqRAEMmVEOC22zwELrus/PsDBsCll8I990BxVkvKiEShQBDJlZdegvffhx//GJrt9U/rn/8Zdu+Gu++OU5tIFhQIIrny4INw8MFw/vmV93XsCGPHwu9+B9u2pV6aSDYUCCK5sGMHTJ4M3/oWtG5d9WuuvBI2bYJJk9KtTSRLCgSRXJgyBbZsge98p/rXDBniLYXHH0+vLpE6UCCI5MLkyXDQQTBiRPWvadbMA2PKFNi4Mb3aRLKkQBBpqBBg6lQYORJatqz5teedB198AU89lU5tInWgQBBpqEWLYMUKOOWU2l/bvz+0bw/PP598XSJ1pEAQaaipU307alTtrzWD0aN9KYtdu5KtS6SOFAgiDTVtGnTuDN26Zff60aN9APqNNxItS6SuFAgiDRECvPmmX0GUrcxYg7qNJM8oEEQaorgY1q2Dr30t+/cceCD83d95t5FIHlEgiDTEjBm+rUsgAAwf7stc6PJTySMKBJGGePtt2HdfX9CuLoYN8+6mV19NpCyR+lAgiDTEjBlQVAQt6nivqQEDPEhefjmZukTqQYEgUl+7dsGcOXXvLgJo1QoGDYLp03Nelkh9KRBE6mv+fA+F/v3r9/5hw/yOap98ktOyROpLgSBSX3Pn+vb44+v3/mHDfPvaazkpR6ShFAgi9TV3ro8DHH10/d5fVOTzEd56K7d1idSTAkGkvubOheOOg+bN6/f+1q2hX7/yS1dFIlMgiNRHCB4I9e0uyhg4EGbO9BVQRSJTIIjUx+rVPhici0DYvt0Hl0UiUyCI1EfmAzwXgQAaR5C8oEAQqY/MFUZ1naG8t44d4cgjFQiSFxQIIvUxbx506OC3zWwIM28lKBAkDygQROpj4ULo2TM3xxo4EJYvh7Vrc3M8kXpSIIjUVQi5DYTM0hd//WtujidSTwoEkbpavRq2boVjj83N8fr1g2bNYNas3BxPpJ4UCCJ1tXChb3MVCPvtB717KxAkuuiBYGbNzewdM3s2di0iWVmwwLe5CgTw5bBnzvTuKJFIogcCcA2wIHYRIllbuBDatIHDD8/dMYuKYMMGKCnJ3TFF6ihqIJhZB+AM4IGYdYjUycKF3jowy90xBwzwrbqNJKLYLYTfAP8I7K7uBWY23sxmmdms9evXp1eZSHUygZBLffr4yqczZ+b2uCJ1EC0QzGwM8HEIYXZNrwsh3B9CKAohFLVr1y6l6kSqsWULrFqV+0Bo1Qr69lULQaKK2UIYDJxlZh8BjwEjzOzhiPWI1O7DD32b60AA7zaaNQt2V9tgFklUtEAIIdwUQugQQugMnAe8FEK4KFY9IllZtMi3PXrk/thFRbB5Myxdmvtji2Qh9hiCSGHJfFh37Zr7YxcV+VbdRhJJXgRCCGF6CGFM7DpEarVkCbRv77fOzLXevf0uahpYlkjyIhBECsbSpdCtWzLHbtHCl7FQC0EiUSCI1EWSgQDebTRnDnz5ZXLnEKmGAkEkW59/7ktUd++e3DkGDPDzZNZLEkmRAkEkW8uW+TbpFgKo20iiUCCIZGvJEt8mGQjHHAMHHKCBZYlCgSCSrcwlp0kGQrNm0L+/WggShQJBJFtLl8LBB/sjSUVF8O67sGtXsucR2YsCQSRbS5cmO6CcMWAA7NwJ8+Ylfy6RChQIItlK+pLTDA0sSyQKBJFsfPEFFBenEwhdu3q3lAaWJWUKBJFsFBf7ZLE0AsHMWwlqIUjKFAgi2UjjCqOKBgyA99+HHTvSOZ8ICgSR7Hz0kW+7dEnnfEVFUFoKc+emcz4RFAgi2SkpgebN4cgj0zlfZmBZ4wiSIgWCSDaKi6FDBw+FNHToAIcdpnEESZUCQSQbJSVw1FHpnU8DyxKBAkEkGyUl0KlTuuccMAAWLICtW9M9rzRZCgSR2pSWwsqV6bYQwFsIu3fDO++ke15pshQIIrVZs8bnIKTdQtDAsqRMgSBSm+Ji36YdCIcdBh07ahxBUqNAEKlNSYlv0+4yAg0sS6oUCCK1yQRCx47pn3vAAFi8GD79NP1zS5OjQBCpTXExtG3rdzJLW2YcYfbs9M8tTY4CQaQ2ac9BqEgDy5IiBYJIbYqL0x9Qzjj4YF9QT+MIkgIFgkhNQvBAiNVCAB9H+Otf451fmgwFgkhNNm/2mcKxWggAJ58MK1bAqlXxapAmQYEgUpNYcxAqGjzYt2++Ga8GaRIUCCI1iTkHIeOEE2DffRUIkjgFgkhN8qGF0LIlnHQSvPFGvBqkSVAgiNSkpARatYJDD41bx6BBvsjdtm1x65BGTYEgUpOSEp+h3CzyP5XBg33VVc1HkAQpEERqEnMOQkUDB/pW4wiSIAWCSE1izlKuqG1b6NlT4wiSKAWCSHV27fJ7IeRDCwF8HOGtt/ymOSIJUCCIVGflSp+pnA8tBPBxhI0bYeHC2JVII6VAEKlOZg5CvrQQhg3z7csvRy1DGi8Fgkh18mEOQkVdukDnzjBtWuxKpJFSIIhUJ+aNcaozYgRMn+73eBbJsWiBYGYdzexlM1tgZvPM7JpYtYhUqbjY72vcunXsSsqNGAGbNsHcubErkUYoZguhFLguhNAT+BrwQzPrFbEekT3lyyWnFY0Y4duXXopbhzRKLWKdOISwBlhT9nyLmS0A2gPzc36yp57yy/UAzHK3zeWx6rtt3hz2288f++5b/ny//eCQQ6BdO196QequuBj69o1dxZ6OOMLnI0ybBtdfH7uaZOza5feQ/uyzPR87dsAXX/j+zDbzvLTUrwir7QHVfz/fXXopHHNMoqeIFggVmVlnoB/wdhX7xgPjATrVd3Bv+nS47749fyHqsy1UBx3kXR9HHQXHHuu/VH36+O0Z9903dnX5KQRvIYwZE7uSykaMgAcf9A/DffaJXU39rFoF8+bBggX+KC6G1av9sWFD7s+X+QNu7z/o9v5+Phs5MvFAsBD5w87MDgBeAX4RQphU02uLiorCrNi3EmxoqOR6W1oK27f7omeZx/btflOXTz6BdevKH8uX+zXsW7f6e1u29FA49VT49rehd+/C+IeRhvXrfUG73/4Wrr46djV7mjwZzj7bu42GD49dTXaKi+HZZ+G113y29cqV5fsOPhi6doX27eHII70V9NWvQps28JWv+PbAA/2Pl5YtPQT33jZvXvWHvH6fATCz2SGEotpeF7WFYGYtgYnAI7WFQd7Yu9um0ITgf4XNmeP/MF99FW69FW65xQPhqqvg4oth//1jVxpXvl1yWtHIkd4N+Oyz+R0I69bB//4vPPGEr9QK0KGDT7AbNMi743r18m7NQv331MhEayGYmQG/BzaGEK7N5j150UJojNauhSefhN/9DmbP9nVzJkyAH/ygcLskGmrSJDjnHA/Ofv1iV1PZ6afDsmXw4YexK6ls1iy4/Xb/nSot9YX5vvUt+OY3oUeP2NU1Sdm2EGJeZTQYuBgYYWbvlj1GR6yn6Tr8cP/wnzkTXn8d+veHa6/1cYbMYHxTk88tBIAzzoBFi/yRL959F848EwYMgKlTvatt4UJfofWGGxQGBSBaIIQQXg8hWAihbwjhhLLH87HqEbzZPngwTJkCzz3ng5Zf/zrcfHPTW1CtpMSv1GrbNnYlVTvjDN8+91zcOsCvCPrRj/wPiTfegJ//HD76CP7jPxIfBJXc0kxlqcwMRo/2yU8XXeTjC+ee27Tu1pWZg5CvfdtduviYz9NPx63juef8yrX/+i+48krvxvrpT30gWAqOAkGq16aNX9746197f/App8CWLbGrSke+3BinJmPHwiuv+BLdaduxw1sFY8Z4l+PMmXDXXX6JsxQsBYLUzMzHE/7v/+Dtt/0D4PPPY1eVvHycpby388/3q8aeeCLd85aU+EDx3XfDj3/svxcnnphuDZIIBYJk55xz4JFHfND5wgsb95jCtm0+DyHfWwg9e8Lxx8Mf/5jeOd96yweNly3zy17vuEMz4RuRrALBzA41s2+Z2Q/N7FIzO8nMFCZNzd//vXcfTZ7sl6U2VitW+DbfAwG8lfD22/4BnbRHH/V7Mhx4IMyYUT6wLY1GjR/qZjbczKYAzwGnA0cAvYCfAe+b2S1mptGjpuRHP4Lvfx9+8Qv/C7Exyix7ne9dRuCBYOZjPUm65x5vGQ4c6AHUs2ey55MoavsrfzRweQhhQAhhfAjhZyGE60MIZwHHA+8ApyRepeQPM+87Pv54uOwy+Pjj2BXlXr7PQaioUyefpPbf/+2LvCXhttv8CqIzz4QXXvBlJaRRqjEQQgg3hBBKqtlXGkJ4KoQwMZnSJG+1agUPPwybN8Pllxf+4n97KymBZs18bZ1CcOWVPtv8qadye9wQ4Kab/HH++TBxYn7dG0JyLtsxhIfM7CsVvu5sZrqPX1N23HE+Aenpp+NfC59rxcW+yFrLlrEryc5pp/mtNe+8M3fH3L3b17W67Ta44gp46KHC+e8h9ZbtwPDrwNtmNtrMLgf+AvwmubKkIFxzjU+OuuaaxjVpraSkMLqLMpo3h5/8xK8Amz694ccrLYXvfc8nm91wg48fNG/e8ONK3ssqEEII9wHfByYDtwJDQgjPJFmYFICWLf1Do7jY/5JsLAphDsLeLr/cl42++eaGHWfXLjjvPPjDH+Bf/xV+9av8na0tOZdtl9HFwP8AlwAPAs+b2fEJ1iWFYsgQ+M53/Hr0xjDAvHu3X3ZaSC0E8L79G2/0mcv17cLbvt3vszBxol9e/LOfKQyamGy7jM4Bvh5C+GMI4SbgH/BgEPG/JHfsgH/7t9iVNNzatX61TqEFAviKtb17+6XBdZ1NvnGjj0W88ALcf7/PTpcmJ9suo7NDCB9X+PqvwMmJVSWFpUcPGDfO+5pLqrworXBkLjkttC4j8C68e+/1/wdXXZX91V9Ll/r8ghkzfPLZ5ZcnW6fkrdompv3MzKpc/zeEsMvMRphZHt50VlI3YYJ3t9xxR+xKGiYTaIXYQgBfrnzChPJFCWvz5JNw0kl+u9Vp03z8QJqs2m6h+T7wjJntAOYA64HWwNHACcBUoBH0E0iDdeoEF1zgE6QmTMjf+wjUppBmKVdnwgT44AO47jq/V8GECdBir3/qq1b5/IKHHvL7aj/2GHTrFqdeyRu1dRl9O4QwGJgCzAOaA58BDwMnhRB+HEJYn3CNUiiuv94vP73nntiV1F9xcfmN3QtV8+b+AT9unI/v9O3rt7R86im/Ter55/uH/+OP+70L3nhDYSBALfdUNrP5+BpGTwOV7uYdQtiYXGmV6Z7KBeD00/0+xMXFhTmr9ayz/G5f770Xu5LcmDTJ152aM6f8e23beihcd53faEcavWzvqVxbl9G9wAtAV6DiJ7EBoez7IuV+8hM49VS/dPHCC2NXU3fFxYXdXbS3sWP9sW6ddxO1aQNdu/rSHCJ7qW0toztDCD2B/wkhdK3w6BJCUBhIZSNHevfDfffFrqR+Cm2WcrYOO8xvYtO9u8JAqpXtZac/SLoQaSSaNYPx4+G112D+/NjV1M1nn/kgbGNqIYjUgf5UkNwbN86vib///tiV1E2hX3Iq0kAKBMm9Qw/1W27+4Q+wc2fsarKnQJAmToEgyRg3DjZtguefj11J9grpxjgiCVAgSDJGjvSBzIcfjl1J9oqLfQLXEUfErkQkCgWCJKNFC18G4dlnfaC2EBQXQ8eOWvtfmiwFgiTnoot8ff0//Sl2JdkpxPsgiOSQAkGS078/HHNM4XQbNbZJaSJ1pECQ5Jj5Egmvvur3Gchnu3bB6tUKBGnSFAiSrHPO8XX5J0+OXUnNVq70OhUI0oQpECRZvXvD0Uf7uvv5rJBvjCOSIwoESZaZL642bVp+X22kOQgiCgRJwdixUFrql6Dmq8ws5Y4d49YhEpECQZJXVAQdOvja/PmquBgOP7ww7+EgkiMKBEles2Zw9tnwwguwfXvsaqqmS05FFAiSkjPP9DCYPj12JVVTIIgoECQlQ4bAfvvl52J3u3drlrIICgRJS+vWvuDd88/79f75ZN06n5imQJAmLmogmNlpZvahmS0xsxtj1iIpGD0ali2DRYtiV7KnzBVGCgRp4qIFgpk1B/4TOB3oBZxvZr1i1SMpOP103+Zbt5HmIIgAcVsIJwFLQgjLQgi7gMeAb0asR5J21FE+czlfA0EtBGniWkQ8d3tgRYWvVwInJ3Wyq66CFSv2/F6/fnDzzf78sstgw4Y99w8aBP/0T/78ggvg88/33D9yJFx9tT8fOxa+/HLP/WPGwOWXe/f0uedWruncc32F6M2b4ZJLKu+/5BJfCmjtWrjiisr7r7jCe2GWL4drr628/9prYfhwWLAAbqyiQ+7GG2HgQJgzB265pfL+W26BE06AN96A22+vvP/2230x06lT4a67Ku+/6y7/o/uZZ+CBB8q+ufNxmLYMzijlgQdb0K4dPPEEPPJI5fc/8ggccAD8/vdVT2GYONFvu3DvvfDnP++5r2XL8lW3f/3ryhc3tWkDDz3kz3856RhmtHgOLvnK3/Yfdlj5LaEnTIC5c/d8f+fO8Nvf+vMbbqjcC3bssfCrX/lz/e5V3h/ld6+CBx4gP373fgkzZuy5v6rfvbFj4bvfrVxHrsUMBKvie5VGG81sPDAeoFMDmvRr1pR3FWd06FD+fPXqygty9uhR/nzlStiyZc/9GzeWP1+xwifjVrRpk29DqHxuKF/JIXORy942b/ZtaWnV+zP17NpV9f6tW327c2fV+zNTArZvr3r/jh2+3bat6v2Z2yVv3Vr1/i++KK+zfH8nCLtgweeUlvoH8GefVf3+3bt9u3lz1fszY9ObNlXev88+5c83bqy8/6CDyp+vX7ebkhZdocJrKn7Afvxx5fdXnL+2bl3l/QcfXP5cv3uV98f53SuX+e8V/XdvfeX9Vf3uZf5/JM1CpCs+zGwgcHMI4RtlX98EEEL4ZXXvKSoqCrNmzUqpQknEzp3+afn978Odd8auxvXt63/yP/107EpEEmFms0MIRbW9LuYYwkzgaDPrYmb7AOcB+hfZ2LVq5XMSpk2LXUk5zUEQASIGQgihFLgKmAIsAJ4IIcyLVY+kaNQomD/f+0pi27zZH7rCSCTuPIQQwvMhhB4hhG4hhF/ErEVSNHKkb/OhlbBsmW+7do1bh0ge0ExlSd/xx8NXv+qXiMS2fLlvFQgiCgSJoFkzbyVMmxZ/GYtMC6FLl7h1iOQBBYLEMWoUrFoFH34Yt45ly/yqp4rXAoo0UQoEiSMzjhC722j5crUORMooECSOrl39gzh2ICxbpvEDkTIKBIln1Cif17/3ugtp2b0bPvpIgSBSRoEg8Qwb5nMA9l4oKC2rV/vaC+oyEgEUCBLT0KG+feWVOOfXHASRPSgQJJ727aF793j3WVYgiOxBgSBxDR0Kr71WvrxkmpYvBzMtWyFSRoEgcQ0d6msIv/9++udetgw6dtxzvWKRJkyBIHFlxhFidBvpklORPSgQJK5OnfwqnxgDy5qUJrIHBYLEN3QovPpquuMI27b5rczUQhD5GwWCxDdsGHzyCcxL8XYYWuVUpBIFgsQXYz7C4sW+Pfro9M4pkucUCBJf584+lpDmwPKiRb5VIIj8jQJB8sOwYT6OkNb9ERYvhnbttOy1SAUKBMkPQ4fC+vWwYEE651u0CHr0SOdcIgVCgSD5Ie35CIsXq7tIZC8KBMkPXbtChw7pDCxv2eKXnKqFILIHBYLkBzMfR5g+PflxBF1hJFIlBYLkj2HD4OOPYeHCZM+TCQS1EET2oECQ/DFsmG+THkfIXHLavXuy5xEpMAoEyR+ZcYSkA2HxYj/Pfvslex6RAqNAkPyR1jiCLjkVqZICQfJL0uMIIcCHHyoQRKqgQJD8kvQ4wpo18Omn0Lt3MscXKWAKBMkvmXGEl19O5vjz5/u2V69kji9SwBQIkl+SHkfILLGtFoJIJQoEyT/DhiW3rtH8+dC2LRx6aO6PLVLgFAiSf4YP920S4wjz5nnrwCz3xxYpcAoEyT9dukDHjrkPhBC8haDxA5EqKRAk/yQ1jrB2LWzapPEDkWooECQ/JTGOkLnCSIEgUiUFguSnJOYjfPCBb9VlJFIlBYLkp8w4Qi7nI7zzDhx2GBx+eO6OKdKIKBAkPyUxjvDOO9CvX26OJdIIKRAkfw0fDhs2wHvvNfxYO3f6GIICQaRaUQLBzP7dzBaa2Xtm9qSZHRSjDslzp57q2ylTGn6sDz6A0lIFgkgNYrUQXgSOCyH0BRYBN0WqQ/JZ+/bQp09uAuGdd3x74okNP5ZIIxUlEEIIfwkhlJZ9OQPoEKMOKQDf+Aa89hps3dqw48yZA23a+GC1iFQpH8YQLgX+HLsIyVOnnQZffNHwy0/nzIETToBm+fArL5KfEvvXYWZTzeyDKh7frPCanwKlwCM1HGe8mc0ys1nr169PqlzJV1//ut/qsiHdRjt3epfRySfnri6RRqhFUgcOIYyqab+ZfRcYA4wMofrrCkMI9wP3AxQVFSV4X0XJS61a+dVGL7xQ/2PMmQO7dsHAgbmrS6QRinWV0WnAPwFnhRC2xahBCsg3vgFLlsDSpfV7/5tv+laBIFKjWB2qdwMHAi+a2btmdm+kOqQQjB7t22eeqd/733zTB5M1Q1mkRrGuMuoeQugYQjih7PEPMeqQAtGtG/TtC5Mm1f29IXggDBqU+7pEGhldciGFYexYeP11WLeubu9bssSXvR48OJm6RBoRBYIUhrFj/a/9yZPr9r6pU307qsZrHEQEBYIUiuOO866junYbvfgiHHUUdO+eTF0ijYgCQQqDmbcSpk3zG+dk48sv4aWXvHWgeyiL1EqBIIXj4ot9gbpHH83u9TNmwObN5YvkiUiNFAhSOPr0gf794cEHs3v9xImwzz6+/IWI1EqBIIVl3Dh4911/1CQED4RTT/VF7USkVgoEKSwXXACtW8Pdd9f8uhkzoKQEzjknnbpEGgEFghSWtm3he9+Dhx6CNWuqf91998EBBygQROpAgSCF57rrfHD5jjuq3r9hAzz+OFx4IRx4YLq1iRQwBYIUnm7d4KKL4M47fSby3m6/3Ze8vvrq9GsTKWAKBClMt93mS2OPH++thYyFC+Guu7x10KtXvPpECpACQQrTEUd4C+Hllz0Udu6EVat8zGD//b2VICJ1ktgNckQSN24cLF8Ot94KTz4J27dD8+bw3HMeGCJSJwoEKWy33AJDhsBjj/lVRVdeCUcfHbsqkYKkQJDCN3KkP0SkQTSGICIigAJBRETKKBBERARQIIiISBkFgoiIAAoEEREpo0AQERFAgSAiImUshBC7hqyZ2XqgOHYd9XAIsCF2ESlqaj8v6GduKgr1Zz4qhNCuthcVVCAUKjObFUIoil1HWprazwv6mZuKxv4zq8tIREQABYKIiJRRIKTj/tgFpKyp/bygn7mpaNQ/s8YQREQEUAtBRETKKBBSZGbXm1kws0Ni15I0M/t3M1toZu+Z2ZNmdlDsmpJiZqeZ2YdmtsTMboxdT9LMrKOZvWxmC8xsnpldE7umNJhZczN7x8yejV1LUhQIKTGzjsApQEnsWlLyInBcCKEvsAi4KXI9iTCz5sB/AqcDvYDzzaxX3KoSVwpcF0LoCXwN+GET+JkBrgEWxC4iSQqE9Pwa+EegSQzahBD+EkIoLftyBtAhZj0JOglYEkJYFkLYBTwGfDNyTYkKIawJIcwpe74F/5BsH7eqZJlZB+AM4IHYtSRJgZACMzsLWBVCmBu7lkguBf4cu4iEtAdWVPh6JY38w7EiM+sM9APejltJ4n6D/0G3O3YhSdI9lXPEzKYCh1ex66fA/wNOTbei5NX0M4cQJpe95qd4F8MjadaWIqvie02iFWhmBwATgWtDCJ/FricpZjYG+DiEMNvMhsWuJ0kKhBwJIYyq6vtm1gfoAsw1M/CukzlmdlIIYW2KJeZcdT9zhpl9FxgDjAyN9/rmlUDHCl93AFZHqiU1ZtYSD4NHQgiTYteTsMHAWWY2GmgNtDGzh0MIF0WuK+c0DyFlZvYRUBRCKMQFsrJmZqcBdwBDQwjrY9eTFDNrgQ+ajwRWATOBC0II86IWliDzv2x+D2wMIVwbu540lbUQrg8hjIldSxI0hiBJuRs4EHjRzN41s3tjF5SEsoHzq4Ap+ODqE405DMoMBi4GRpT9v3237K9nKXBqIYiICKAWgoiIlFEgiIgIoEAQEZEyCgQREQEUCCIiUkaBICIigAJBRETKKBBEGsDMBpTd86G1me1fdn+A42LXJVIfmpgm0kBm9nN8jZt9gZUhhF9GLkmkXhQIIg1kZvvgaxjtAAaFEL6MXJJIvajLSKTh2gIH4Gs3tY5ci0i9qYUg0kBm9jR+p7QuwBEhhKsilyRSL7ofgkgDmNklQGkI4dGy+yu/aWYjQggvxa5NpK7UQhAREUBjCCIiUkaBICIigAJBRETKKBBERARQIIiISBkFgoiIAAoEEREpo0AQEREA/j/qBE4RAAAABElEQVSy+TtArSOH4QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f057d79b590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def funk(x):\n",
    "    poly = 3.0 + 5.0*x - 10.0*(x**2.0) + 1.0*(x**3)\n",
    "    expo = np.exp(-(x**2.0))\n",
    "    f = poly * expo + 2\n",
    "    return f\n",
    "\n",
    "x = np.linspace(-5,5,10000)\n",
    "f = funk(x)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.plot(x,f,linestyle='-', color='red')\n",
    "zero = 0.0*x\n",
    "plt.plot(x,zero,linestyle='--', color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Write a function that takes *funk()* and a desired level of accuracy (in $x$) as an input and uses a brute-force gridding algorithm to return the location and value of the (global) minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2.  iterative improvement\n",
    "\n",
    "We can start with one or more trial locations and then iteratively move to better ones (e.g. by replace the highest point with a new, better estimate)\n",
    "\n",
    "* key challenge is to generate new trial locations that yield better estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* good example: the famous \"Nelder-Mead\" or \"simplex\" algorithm\n",
    "    * a \"simplex\" is just a collection of $d+1$ points    \n",
    "    * capable of \"enclosing\" a specific location in $d$-dimensional parameter space    \n",
    "        * e.g. for $d = 1$, simplex consists of 2 points, which can enclose $x_{min}$\n",
    "        * trial locations based on moves representing\n",
    "            * reflection\n",
    "            * expansion\n",
    "            * contraction\n",
    "        * picture this like an amoeba moving through parameter space       \n",
    "        * simplex \"oozes\" towards and contracts around the minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* advantages\n",
    "    * very good general-purpose algorithm, even in higher dimensions\n",
    "    * no need for derivatives\n",
    "<br><br>\n",
    "* disadvantages\n",
    "    * can get fooled by local minima\n",
    "    * can occasionally also fail on problems that \"should\" be solvable\n",
    "    * general convergence properties not known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "##### Exercise\n",
    "\n",
    "Write a function that takes *funk()*, along with two bracketing locations $x_1$ and $x_2$ as input and uses bisection to successively replace the location corresponding to the higher value of $f(x)$ with a new location suggested by bisection (until this new location no longer corresponds to a lower $f(x)$ or until some pre-set level of accuracy has been reached).\n",
    "\n",
    "Note: we have not covered bisection yet, but it simply means taking the half-way point between two existing points. So, given $x_1$ and $x_2$, your function should choose as its next trial location $x_{try} = x_1 + 0.5*x_2$.\n",
    "\n",
    "Try the algorithm for a range of different initial bracketing locations $x_1$ and $x_2$. Also test to see what happens when $x_1$ and $x_2$ do not bracket the minimum (or bracket only the local minimum).\n",
    "\n",
    "How well does this algorithm work? Think about the ways in which it can fail and whether/how these are dealt with by the simpex algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 3. use derivatives to step downhill\n",
    "        \n",
    "We can try using (numerical or analytical) derivatives in order to consistently move \"downhill\".\n",
    "\n",
    "* a key issue for such methods is how far to step in the direction of the gradient<br><br>\n",
    "* advantages\n",
    "    * can be very efficient when derivatives are known and cheap\n",
    "<br><br>\n",
    "* disadvantages\n",
    "    * requires derivatives\n",
    "    * can be inefficient for some functions\n",
    "        * local \"downhill\" direction does not have to point towards minimum           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### practical recommendations\n",
    "\n",
    "I recommend using brute-force gridding whenever it's viable to do so. It's the simplest, most robust and easiest to understand method.\n",
    "\n",
    "If brute-force gridding is not viable (usually because there are too many dimensions to explore), I would recommend trying the simplex algorithm. This is a nice, general-purpose method that often does surprisingly well, even in situations where one might not expect it to.\n",
    "\n",
    "Finally, whenever you minimize a function, be aware of the limitations of most numerical minimizers. Most importantly, with the exception of brute-force gridding, they are all susceptible to getting stuck in local minima. \n",
    "\n",
    "*Always sanity-check your function and your results!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### minimization with *scipy*\n",
    "\n",
    "A general-purpose miminization function called <a href=\"http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize\"> minimize()</a> is included in the <a href=\"http://docs.scipy.org/doc/scipy/reference/optimize.html\">optimize</a> module within *scipy*.\n",
    "\n",
    "The *minimize()* function includes support for both iterative-improvement algorithms (e.g. Nealder-Mead simplex and Powell's method) and derivative-based methods (such as the \"conjugate gradient\" and \"quasi-Newton\" algorithms).\n",
    "\n",
    "The type of algorithm to be used is passed to the *minimize()* as an input parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise\n",
    "\n",
    "Use *scipy's* *curve_fit()* to fit a linear relation to the provided galaxy redshift data (in hubble.dat file). Plot the fitted model over the data, error bars included. What value for Hubble's constant does your fitted model give? The following function may be useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def polynomial_relation (params, x, n):\n",
    "    if len(params) == n + 1:\n",
    "        y = np.zeros(len(x));\n",
    "        for i,param in enumerate(params):\n",
    "            y = y + param * x**i\n",
    "        return y\n",
    "\n",
    "    else:\n",
    "        print (\"Aborting: Not enough paramters provided\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise\n",
    "\n",
    "Use *scipy's* *minimize()* to fit a linear relation to the provided galaxy redshift data (in hubble.dat file) using $\\chi^2$ minimization. Plot the fitted model over the data, error bars included. What value for Hubble's constant does your fitted model give? Now fit a higher order (n>1) polynomial and plot it on the same graph. Evaluate the goodness of fit of both models using the reduced $\\chi^2$ statistic. The following function may be useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chi2_polynomial(params, xdata, ydata, error, n):\n",
    "    \n",
    "    modelvals = polynomial_relation(params, xdata, n)\n",
    "    \n",
    "    return np.sum((ydata - modelvals)**2.0/(error**2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## interpolation\n",
    "\n",
    "It is extremely common that the outcome of an experiment or an observation consists of *samples* of some function $y = f(x)$, i.e. we are given a fixed number of pairs of $(x_i, y_i)$. We do not know what the underlying continuous function is (if we did, we wouldn't have needed to do the experiment).\n",
    "\n",
    "In such cases, it is almost inevitable that we will need to numerically estimate the value of the function at some new location $x$, which is not part of the known set of $x_i$ values.\n",
    "\n",
    "If neither $x_i$ nor $y_i$ are subject to significant errors/uncertainties, and if $f(x)$ is known to be a smooth, continuous function, we typically obtain such estimates by *interpolating* on the known values:\n",
    "\n",
    "* **Interpolation** here refers to the construction of some continuous function that:\n",
    "    * goes through all of the known data points\n",
    "        * $g(x_i) = y_i$\n",
    "    * (hopefully) provides a reasonable approximation to $f(x)$ *between* the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### nearest-neighbour interpolation\n",
    "\n",
    "The simplest possible interpolation algorithm is *nearest-neighbour* interpolation.\n",
    "\n",
    "In nearest-neighbour interpolation, our estimate of $f(x_{try})$ is simply $f(x_i)$, with $x_i$ chosen such that it is the closest point on the grid to $x_{try}$.\n",
    "\n",
    "Nearest-neighbour interpolation doesn't really have any significant advantages over linear interpolation, which we'll cover next, other than perhaps being even faster. We won't deal with it any further here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### linear interpolation\n",
    "\n",
    "Perhaps the most obvious way to interpolate between two data points $(x_1, y_1)$ and $(x_1, y_2)$ is to connect them with a straight line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This corresponds to taking \n",
    "<div align=\"center\"> \n",
    "$g(x) = y_1 + \\left(\\frac{x - x_1}{x_2 - x_1}\\right) (y_2 - y_1)$ \n",
    "</div>\n",
    "\n",
    "in the interval $x_1 \\leq x \\leq x_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Linear interpolation for a set of more than two data points then simply requires connecting each successive pair by a straight line.\n",
    "\n",
    "<div align=\"center\"><img height=50% width=50% src=\"lininterp.png\"> </img></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "##### Exercise (optional)\n",
    "\n",
    "First make sure you understand the equation describing linear interpolation above. Then implement a function that carries out linear interpolation on an arbitrary function defined on an arbitrary (sorted) grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A useful alternative way to think about linear interpolation is to think of it as a weighted average of the two bracketing data points. \n",
    "\n",
    "We can see this more clearly by writing\n",
    "<div align=\"center\">\n",
    "$g(x) = w_1 y_1 + w_2 y_2$.\n",
    "</div>\n",
    "\n",
    "A little algebra shows that this is equivalent to our linear interpolation equation if the weights are\n",
    "<br><br>\n",
    "<div align=\"center\">\n",
    "$w_1 = 1 - \\frac{x - x_1}{x_2 - x_1}$\n",
    "<br><br> \n",
    "$w_2 = 1 - \\frac{x - x_2}{x_2 - x_1}$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "##### Exercise (optional)\n",
    "\n",
    "Carry out the steps required to transform the original equation we had for linear interpolation into the \"weighted average\" notation shown above. Make sure you understand why the weights are eminently reasonable: each known point is weighted linearly according to its distance from the trial point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This representation is useful because it generalizes easily to higher dimensions. For example, suppose we want to carry out *bilinear* interpolation on a 2-D function $z = f(x,y)$, for which we have samples on a 2-D grid of data points, $(x_i, y_i)$.\n",
    "\n",
    "Bilinear interpolation can be thought of as simply carrying out successive linear interpolations. That is, in order to find an estimate for $z(x,y)$, we first carry out a 1-D linear interpolation (say in $x$) to obtain estimates for $z(x,y_1)$ and $z(x,y_2$). We then obtain our estimate of $z(x,y)$ by interpolating linear between these two along the $y$ direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's see how the weighted average form of the 1-D linear interpolation equation makes it easy to derive the equation for *bilinear* interpolation.\n",
    "\n",
    "Looking at the the weighted average form of the 1-D equation, it is immediately clear that our interpolating function will have the form:\n",
    "\n",
    "<div align=\"center\">\n",
    "$g(x) = w_{11} \\, z(x_1,y_1) + w_{12}\\,  z(x_1,y_2) + w_{21}\\,  z(x_2,y_1) + w_{22}\\,z(x_2,y_2)$\n",
    "</div>\n",
    "\n",
    "All this says is that our estimate will be a weighted average of the points defining the grid square that encloses our trial point. The only question is: what should the weights $w_{ij}$ be?\n",
    "\n",
    "Looking again at the weighted average equation for 1-D linear interpolation suggests we might want to use:\n",
    "<br><br>\n",
    "<div align=\"center\">\n",
    "$w_{11} = (1 - \\frac{x - x_1}{x_2 - x_1})(1 - \\frac{y - y_1}{y_2 - y_1})$\n",
    "<br><br> \n",
    "$w_{12} = (1 - \\frac{x - x_1}{x_2 - x_1})(1 - \\frac{y - y_2}{y_2 - y_1})$\n",
    "<br><br>\n",
    "$w_{21} = (1 - \\frac{x - x_2}{x_2 - x_1})(1 - \\frac{y - y_1}{y_2 - y_1})$\n",
    "<br><br> \n",
    "$w_{22} = (1 - \\frac{x - x_2}{x_2 - x_1})(1 - \\frac{y - y_2}{y_2 - y_1})$.\n",
    "</div>\n",
    "\n",
    "We can write this more compactly as\n",
    "<br><br>\n",
    "<div align=\"center\">\n",
    "$w_{ij} = (1 - \\frac{x - x_i}{x_2 - x_1})(1 - \\frac{y - y_j}{y_2 - y_1})$.\n",
    "</div>\n",
    "<br><br>\n",
    "This form can be generalized straightforwardly to $d > 2$ dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "##### Exercise (advanced, optional)\n",
    "\n",
    "Implement a bilinear interpolation function in *Python*. A reasonable implementation might take three 2-D arrays defining $x_i$, $y_i$, $z_i$. You may want to look back at our discussion of the *numpy* function *meshgrid()* to help you set up such arrays. Your function will also have to take as input a trial point, $(x_{try}, y_{try})$, and should then return an estimate of $z(x_{try}, y_{try})$ based on bilinear interpolation. \n",
    "\n",
    "Note that the first step in your routine will have to be the identification of the four points that bracket $(x_{try}, y_{try})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Linear interpolation is almost always the first thing we tend to rely on when we have to interpolate a function.\n",
    "\n",
    "Its advantages are:<br>\n",
    "* it is easy to understand\n",
    "* it is easy to implement\n",
    "* it is quick\n",
    "* given a \"reasonable\" function and samples, it never produces \"crazy\" results\n",
    "* it produces a continuous function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This is an impressive list, and it justifies the popularity of this simple method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, it also has some disadvantages:\n",
    "* it produces discontinuous first derivatives (at the location of the samples)\n",
    "* for certain types of functions, it may not be as precise as methods.\n",
    "\n",
    "If this matters, we need to employ higher-order interpolating functions, the most common being *cubic splines*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### higher-order interpolation\n",
    "\n",
    "The most common reason to move beyond linear interpolation is that we might require our function to have continuous derivatives (perhaps even higher order derivatives).\n",
    "The\n",
    "\n",
    "In such cases, we have to move beyond a piece-wise linear approximation of our function. The obvious next step is instead approximate it as some sort of higher-order polynomial (or combination of polynomials).\n",
    "\n",
    "There are two basic ways of doing this.\n",
    "\n",
    "#### polynomial interpolation\n",
    "\n",
    "Suppose we have a set of $N$ data point, $(x_i, y_i)$, with $i = 1...N$. We can then obviously construct a polynomial of degree $N-1$ that will go through all of these data points. This is the interpolating polynomial.\n",
    "\n",
    "In practice, direct polynomial interpolation is not widely used. The reason is that when $N$ gets large, the resulting polynomial can -- and usually will -- oscillate wildly between our sample points. This is not good.\n",
    "\n",
    "#### spline interpolation\n",
    "\n",
    "A much more common higher-order interpolation method is to construct a *sequence* of lower-order polynomials (in the same way that linear interpolation is a sequence of first-order polynomials).\n",
    "\n",
    "The idea behind spline interpolation is to connect the points in a way that minimizes the \"bending\" of the resulting curve. The term \"spline\" is the name of a sort of \"bendy ruler\" that was used to draw curves like this in practice.\n",
    "\n",
    "Cubic splines are the most commonly used type of splines. The basic idea behind their construction is that any two sample points $(x_1,y_1)$ and $(x_2,y_2)$ are connected by a 3rd order (cubic) polynomial that goes through both of these points. The two polynomials that \"meet\" at a given $x_i$ are further constrained to have the same first and second derivatives at $x_i$. These constraints define a unique set of polynomial coefficients and also minimize the bendiness of the resulting function.\n",
    "\n",
    "Since cubic splines are smooth combination of just 3rd order polynomials, they do not produce the wild oscillations between data points that are commonly associated with direct (higher-order) polynomial interpolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### interpolation with *scipy*\n",
    "\n",
    "There is a dedicated interpolation module in *scipy*, appropriately named <a href=\"http://docs.scipy.org/doc/scipy/reference/interpolate.html#module-scipy.interpolate\">interpolate</a>.\n",
    "\n",
    "The two main functions we should be aware of in this module are <a href= \"http://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html#scipy.interpolate.interp1d\">interp1d()</a> and <a href=\"http://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.griddata.html#scipy.interpolate.griddata\">griddata()</a>.\n",
    "\n",
    "*interp1d()* provides nearest-neighbour, linear and cubic spline interpolation for 1-dimensional functions.\n",
    "\n",
    "*griddata()* provides the same for multi-dimensional functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "##### Exercise (optional)\n",
    "\n",
    "First familiarize yourself with *interp1d()* and *griddata()*. Then write a program that uses these functions to interpolate 1-D and 2-D functions. Compare the results to your own linear and bilinear interpolation functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
